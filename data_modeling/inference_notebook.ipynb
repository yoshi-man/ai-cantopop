{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79bba78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3760398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read txt file in the local folder, and clean up unwanted characters\n",
    "def read_data(path):\n",
    "    text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "    return text\n",
    "\n",
    "# Get the set of characters within the text, which would eventaully be indexed\n",
    "def get_vocab(text):\n",
    "    vocab = sorted(set(text))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Simple function that takes the list of chars and join them into 1 string\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db633bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our data\n",
    "text = read_data('./reduced_text.txt')\n",
    "vocab = get_vocab(text)\n",
    "\n",
    "\n",
    "# Converters between the \"ids\" and \"characters\", which tokenizes each character based on our vocab pool\n",
    "# Invert just means it the function is to recover our original vocab character based on id\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
    "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
    "\n",
    "# Average characters per line is 6, usual patterns in chinese songs come in stanzas of 4 lines\n",
    "# So we would use information rougly from the previous 3 lines to determine what to write\n",
    "seq_length = 24\n",
    "\n",
    "# The embedding dimension, this was up-ed as there are way more characters in the chinese language,\n",
    "# each being much more information dense\n",
    "embedding_dim = 1024\n",
    "\n",
    "# Number of RNN units, here we'll use LSTM\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41eb430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lyrics_generation_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  4419584   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  multiple                  6297600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4423900   \n",
      "=================================================================\n",
      "Total params: 15,141,084\n",
      "Trainable params: 15,141,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class LyricsGenerationModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "            \n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "model = LyricsGenerationModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.build(input_shape=(seq_length, len(ids_from_chars.get_vocabulary())))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a74ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7734aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f068df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(seed_sentence, length):\n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "    states = None\n",
    "    next_char = tf.constant([seed_sentence])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(length):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "\n",
    "    print(result[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca86adf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好世界也要失去自己\r\n",
      "我為你生死得哭完全沒有答案\r\n",
      "我過想與你亂離將多討厭也像多一分鐘給我傷透的煎熬\r\n",
      "如果交錯了不容易\r\n",
      "如若我還有愛還真你錯愛你\r\n",
      "但願大地我會待你好由你感覺一天\r\n",
      "聽天甜蜜感眼\r\n",
      "\r\n",
      "在你生命裡一分再相伴一切分擔\r\n",
      "我願意背對你心中激盪\r\n",
      "有罪有讓我有你滿身\r\n",
      "當我是知己亦算知錯\r\n",
      "願意今生約定他生再擁抱\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "對白髮夢但求令動物現在\r\n",
      "即使你重新來過\r\n",
      "但說到還記得可\n"
     ]
    }
   ],
   "source": [
    "generate_lyrics('你好世界',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b46cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
